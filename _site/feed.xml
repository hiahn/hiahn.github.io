<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hyungil ahn</title>
    <description>Foo's blog...</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <updated>2018-04-04T01:03:08-07:00</updated>
    <id>http://localhost:4000</id>
    <author>
      <name>Foo Boo</name>
    </author>
    
      <item>
        <title>Bayesian regression with a known function form</title>
        
          <description>&lt;p&gt;&lt;em&gt;Deep probabilistic programming&lt;/em&gt; is a method of performing “Bayesian” probabilistic modeling on deep learning frameworks. It empowers us to make composite and deep network models represent probabilistic uncertainty in predictions.&lt;/p&gt;

&lt;p&gt;Deep probabilistic programming may be characterized by doing Bayesian inference using differentiable programming. Thus, deep probabilistic programming may be called “Bayesian differentiable programming”.&lt;/p&gt;

&lt;h5 id=&quot;why-deep-and-differentiable-programming&quot;&gt;Why Deep and Differentiable Programming?&lt;/h5&gt;

&lt;p&gt;Deep learning frameworks (e.g., PyTorch, TensorFlow, MxNet) allow us to define a target model in a &lt;em&gt;deep&lt;/em&gt; and composite network structure assembling the building blocks of component models (= parametric linear or non-linear functions) that run in data-dependent, procedural and conditional manner. Also, they provide a tool to estimate the model parameters in terms of &lt;em&gt;differentiable&lt;/em&gt; optimization like stochastic gradient decent (SGD) and back-propagation algorithms.&lt;/p&gt;

&lt;p&gt;Since Bayesian modeling is based on a probabilistic model of the generative process relating the observed data with the uncertain latent variables (= generating parameters), it is very desirable to have the representational power of a deep and composite network model to sufficiently describe the potentially complex generative processes with multiple input variables. In addition, the exact computation of the posterior distribution of latent variables based on the prior and the observed data is most likely to be intractable. Thus, this requires approximate Bayesian inference techniques, such as variational inferences (VIs). Thankfully, VIs transform the approximate posterior inference problems into the optimization problems where we optimize the hyperparameters of approximate posterior distribution (often assumed to be a Gaussian distribution)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(\theta | \mu_\theta)&lt;/script&gt;

&lt;p&gt;since differentiable learning solves the&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c = \pm\sqrt{a^2 + b^2}&lt;/script&gt;

&lt;p&gt;supported by deep learning is very critical in keeping the sufficient model representative power of the underlying process. DL models can describe quite flexible and complex generative processes.&lt;/p&gt;

&lt;p&gt;RNN time-sequence modeling + Bayesian&lt;/p&gt;

&lt;h5 id=&quot;why-bayesian-inference&quot;&gt;Why Bayesian Inference?&lt;/h5&gt;

&lt;p&gt;In Bayesian modeling we posit that the model parameters (a.k.a. latent or hidden variables) generating the observed data are &lt;em&gt;uncertain&lt;/em&gt; in our knowledge. Thus, our beliefs about the true values of &lt;em&gt;generating&lt;/em&gt; variables are described by a probability. That is, we use a probability to denote our uncertainty about the hidden variables selected to describe the generating process. Let’s take the example of a Bayesian parametric regression such as
&lt;script type=&quot;math/tex&quot;&gt;y = f(\mathbf{x}; \theta) + \epsilon&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; are the given input vector and the observed output variable (scalar), and &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is the &lt;em&gt;uncertain&lt;/em&gt; latent variables (vector) or the generating parameters described by a probability distribution. There are important modeling assumptions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f(\mathbf{x}; \theta)&lt;/script&gt; is an assumed generating function we specify with random noise &lt;script type=&quot;math/tex&quot;&gt;\epsilon \sim \mathrm{Normal}(0, \sigma^2)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f(\mathbf{x}; \theta)&lt;/script&gt; is a deterministic function for any sampled value of &lt;script type=&quot;math/tex&quot;&gt;\theta \sim \mathrm{Normal} (\mathbf{0}, \sigma_{\theta}^2 \mathbf{I})&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;The level of &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; is assumed to be fixed as a constant value and also related to how accurately we may specify our function &lt;script type=&quot;math/tex&quot;&gt;f(\mathbf{x}; \theta)&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Whereas a non-Bayesian (deterministic) approach views &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; as a fixed variable to be estimated, a Bayesian (probabilistic) approach regards &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; as an uncertain variable whose &lt;em&gt;probability distribution&lt;/em&gt; is to be estimated to explain the observed data. Maximum likelihood (ML) or maximum a posteriori (MAP) estimations are well-known non-Bayesian approaches determining a fixed &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now let’s represent the above Bayesian regression in terms of probability distributions.&lt;/p&gt;

&lt;p&gt;The complete generative process in the Bayesian perspective is always described in the joint probability distribution of all observed and latent variables.
Since &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; is given and &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; is assumed to be fixed  here, the joint distribution for the complete generative process is &lt;script type=&quot;math/tex&quot;&gt;p(y,\theta | \mathbf{x}, \sigma^2)&lt;/script&gt;. Factorizing
&lt;script type=&quot;math/tex&quot;&gt;p(y,\theta | \mathbf{x}, \sigma^2) = p(y | \theta,
\mathbf{x}, \sigma^2) p(\theta)&lt;/script&gt;, we represent the complete generative process in the combination of the likelihood and the prior distributions. It is important to note that the exact forms of the likelihood and the prior distributions are part of our modeling assumptions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;em&gt;likelihood&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;p(y | \theta, \mathbf{x}, \sigma^2)&lt;/script&gt; is our assumed probability model to describe a generating process of the observed variable &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; from a sample of latent variables &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. Assuming that the likelihood is normally distributed with &lt;script type=&quot;math/tex&quot;&gt;f(\mathbf{x}; \theta)&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;y_\mu&lt;/script&gt; (= the expected value of &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;) and &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; as the Gaussian noise level,
&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned}
y \sim p(y | \theta, \mathbf{x}, \sigma^2) = \mathrm{Normal}(y_\mu, \sigma^2 ) = \mathrm{Normal}(f(\mathbf{x}; \theta), \sigma^2 ).
\end{aligned}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that a known deterministic physical model &lt;script type=&quot;math/tex&quot;&gt;y_\mu = f(\mathbf{x}; \theta)&lt;/script&gt; can be easily incorporated into the likelihood.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;em&gt;prior&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;p(\theta)&lt;/script&gt; is our assumed probability model to represent the uncertain information of latent variables &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (= model parameters) before we consider the observed data &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;. The prior probability of model parameters is represented with some hyperparameters. For example, a Gaussian prior with independent parameters can be described with the hyperparameters of prior mean &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\mu}_\theta&lt;/script&gt; and prior variance assumed to be  &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\sigma_{\theta}}^2&lt;/script&gt;. These hyperparmeters are assumed to be known and fixed.
&lt;script type=&quot;math/tex&quot;&gt;\theta \sim p(\theta | \mathbf{\mu}_\theta, \mathbf{\Lambda} ) = \mathrm{Normal} (\mathbf{\mu}_\theta,
\mathrm{diag}(\mathbf{\sigma_{\theta}}^2))&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bayesian posterior inference is to update our belief or probability about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; after observing the data &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;.  Mathematically the Bayes’ rule provides this update rule.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta| y, \mathbf{x}, \sigma^2) = \frac{ p(y, \theta| \mathbf{x}, \sigma^2) } { p(y | \mathbf{x}, \sigma^2) }
= \frac{ p(y | \theta,
\mathbf{x}, \sigma^2) p(\theta | \mathbf{\mu}_\theta,
\mathbf{\Lambda} )} { p(y | \mathbf{x}, \sigma^2) }&lt;/script&gt;

&lt;p&gt;evidence &lt;script type=&quot;math/tex&quot;&gt;p(y | \mathbf{x}, \sigma^2) = \int p(y | \theta,
\mathbf{x}, \sigma^2) p(\theta | \mathbf{\mu}_\theta,
\mathbf{\Lambda} )  \,\mathrm{d} \theta&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;intractable
&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; high-dimensional&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p(\theta| y, \mathbf{x}, \sigma^2)&lt;/script&gt; approximated by another probability distribution &lt;script type=&quot;math/tex&quot;&gt;q(\theta | \mathbf{\mu'}_\theta,
\mathbf{\Lambda'} ) = \mathrm{Normal} (\mathbf{\mu'}_\theta,
\mathrm{diag}( \mathbf{ {\sigma_{\theta}'}^2 }  ))&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Note that the Bayesian posterior inference is to estimate
analysis&lt;/p&gt;

&lt;p&gt;Bayesian probabilistic modeling provides a unified scheme on how to update the probabilistic beliefs (or infer the posterior distributions) about modeling parameters or latent variables using observed data. It also assumes the specification of generative processes (or model functions describing how outputs are produced from inputs) and prior distributions of modeling parameters or latent variables. This specification allows for easy incorporation of prior knowledge into the model form and associated parameter uncertainty.&lt;/p&gt;

&lt;p&gt;Although the initial choice of compared models and associated prior distributions may depend on our domain knowledge about the underlying problems, bayesian reasoning provides an objective scheme to compare different models and priors.&lt;/p&gt;

&lt;p&gt;Bayesian modeling allows us to build more robust and less overfitted models under uncertainty and give probabilistic estimates about target variables in the model. It sounds all good and simple, but a key difficulty in Bayesian probabilistic modeling arises from calculating the posterior distributions for a given complicated model structure and prior. It is very often intractable to compute the “exact” posterior distribution, but the variational inferences (VIs), one of the important methods in deep probabilistic programming, present a commonly-applicable approach to compute the “approximate” bayesian posterior.&lt;/p&gt;

&lt;p&gt;Since VIs transform Bayesian posterior inference problems (i.e., learning uncertain modeling parameters or latent variables) into optimization problems, the SGD optimization in underlying deep learning frameworks can solve the posterior inference problems.&lt;/p&gt;

&lt;p&gt;Again, the Bayesian approach uses a probability to measure the degree of uncertainty of the variables.&lt;/p&gt;

&lt;p&gt;the prior distribution is a Bayesian subjective probability of describing our knowledge about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; in an objective fashion.&lt;/p&gt;

&lt;p&gt;is only described by a &lt;em&gt;subjective&lt;/em&gt; probability. That is,&lt;/p&gt;

&lt;p&gt;the probabilistic model is a generative process describing how latent variables&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;For example, when observation &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is assumed to be generated depending on model parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, we can describe the overall generative process by &lt;em&gt;joint distribution&lt;/em&gt; $$p(y, \theta) = p(y&lt;/td&gt;
      &lt;td&gt;\theta ) p(\theta)&lt;script type=&quot;math/tex&quot;&gt;.  Here,&lt;/script&gt;p(\theta)$$ is the Bayesian specification about the&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;, the prior belief about the value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; before observing &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; may be viewed as Bayesian subjective probability of describing our best knowledge about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; in an objective fashion.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;In addition, the likelihood probability $$p(y&lt;/td&gt;
      &lt;td&gt;\theta)&lt;script type=&quot;math/tex&quot;&gt;assumes a generating process of observation y for a given&lt;/script&gt;\theta$$.  In other words, both prior and likelihood requires modeling assumptions in terms of their forms.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Bayesian modeling is generative.&lt;/p&gt;

&lt;p&gt;The only way to describe&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;(e.g., P(y&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;\mu_y&lt;/script&gt;) the mean &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; of the observation &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;the probabilities we put as a prior or&lt;/p&gt;

&lt;p&gt;Bayesian probabilities about the parameters&lt;/p&gt;

&lt;p&gt;are subjective,&lt;/p&gt;

&lt;p&gt;(e.g., inferring approximate posterior inference)&lt;/p&gt;

&lt;p&gt;conditions (parameterized functions executed on  and perform the model parameter optimization by&lt;/p&gt;

&lt;p&gt;model optimization using “differentiable” modeling&lt;/p&gt;

&lt;p&gt;hyperparameter&lt;/p&gt;

&lt;p&gt;differentiable computing techniques that deep learning provides for   In this manner&lt;/p&gt;

&lt;p&gt;differentiable programming power.&lt;/p&gt;

&lt;p&gt;Deep Learning (frameworks) as a style of computational modeling language to design a composite (hybrid) model of simple building blocks and optimize them in differentiable computation (backprop &amp;amp; SGDs).  This is now called “Differentiable Programming”.&lt;/p&gt;

&lt;p&gt;A deep probabilistic programming framework (e.g., Pyro, Edward) is an extended deep learning framework (e.g., PyTorch, TensorFlow) enabling probabilistic model specifications and Bayesian inferences.&lt;/p&gt;

&lt;p&gt;Deep learning frameworks can be viewed as&lt;/p&gt;

&lt;p&gt;provide a good way to specify a model by combining parametric functional blocks&lt;/p&gt;

&lt;p&gt;networks of&lt;/p&gt;

&lt;p&gt;as the network of parametric functional blocks&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Differentiable Programming is little more than a rebranding of the modern collection Deep Learning techniques, the same way Deep Learning was a rebranding of the modern incarnations of neural nets with more than two layers. But the important point is that people are now building a new kind of software by assembling networks of parameterized functional blocks and by training them from examples using some form of gradient-based optimization. An increasingly large number of people are defining the network procedurally in a data-dependant way (with loops and conditionals), allowing them to change dynamically as a function of the input data fed to them. It’s really very much like a regular progam, except it’s parameterized, automatically differentiated, and trainable/optimizable. Dynamic networks have become increasingly popular (particularly for NLP), thanks to deep learning frameworks that can handle them such as PyTorch and Chainer (note: our old deep learning framework Lush could handle a particular kind of dynamic nets called Graph Transformer Networks, back in 1994. It was needed for text recognition).”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Compared to the original deep learning frameworks that aim to make “deterministic” models for point-estimate predictions, DPP frameworks allow us to 1) specify “probabilistic” models involving the uncertain distributions of model parameters or latent variables, 2) provide approximate Bayesian inferences (e.g., variational inferences) using the powerful stochastic gradient decent algorithms of the original DL frameworks. Thus, DPPs naturally integrate the benefits of bayesian modeling and deep learning.&lt;/p&gt;

&lt;p&gt;Deep learning frameworks provide a tool to specify models in “deterministic” neural-networks and other kinds of functional equations. It is straightforward to specify a hierarchical (mixed, hybrid) model that combines multiple “component” models. Each component model may be based on different sets of feature variables. Non-probabilistic deep learning models do not assume the uncertainty in model parameters (e.g., weights in NN).&lt;/p&gt;

&lt;p&gt;Let’s use Pyro to illustrate how to perform Bayesian regression with a known function form.&lt;/p&gt;

&lt;p&gt;Suppose we’re given a dataset &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; of the form
&lt;script type=&quot;math/tex&quot;&gt;D = {(X_i,y_i)}&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i=1,2,...,N&lt;/script&gt;
The goal of regression is to fit a function to the data of the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i = f(X_i; \theta ) + \epsilon&lt;/script&gt;

&lt;p&gt;Note that function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; can be any known form of deterministic equation or neural network involving the uncertain parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Let’s first implement regression in PyTorch and learn point estimates for the parameters $$\theta. Then we’ll see how to incorporate uncertainty into our estimates by using Pyro to implement Bayesian regression.&lt;/p&gt;

&lt;h5 id=&quot;setup&quot;&gt;Setup&lt;/h5&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# size of toy data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# number of features&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build_linear_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noise_std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noise_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;specify model (prior, likelihood)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;specify inference (form of approximated posterior or variational posterior)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why probabilistic modeling? To correctly capture uncertainty in models and predictions for unsupervised and semi-supervised learning, and to provide AI systems with declarative prior knowledge.&lt;/p&gt;

&lt;p&gt;Why (universal) probabilistic programs? To provide a clear and high-level, but complete, language for specifying complex models.&lt;/p&gt;

&lt;p&gt;Why deep probabilistic models? To learn generative knowledge from data and reify knowledge of how to do inference.&lt;/p&gt;

&lt;p&gt;Why inference by optimization? To enable scaling to large data and leverage advances in modern optimization and variational inference.&lt;/p&gt;

&lt;p&gt;enables Pyro programs to include stochastic control structure, that is, random choices in a Pyro program can control the presence of other random … To enable scaling to large data and leverage advances in modern optimization and variational inference.&lt;/p&gt;

&lt;p&gt;This naturally combines the advantages of two schemes.  The deep learning scheme allows us to specify the model structure (function form)&lt;/p&gt;

&lt;p&gt;specify a model via neural nets or other parameteric funciton forms&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;Y_1,\ldots,Y_N&lt;/script&gt; be &lt;script type=&quot;math/tex&quot;&gt;(d+1)&lt;/script&gt;-dimensional observations (collecting the &lt;script type=&quot;math/tex&quot;&gt;X_n\in\mathbb{R}^d&lt;/script&gt; covariate within each &lt;script type=&quot;math/tex&quot;&gt;Y_n\in\mathbb{R}&lt;/script&gt; response for shorthand) generated from some model with unknown parameters &lt;script type=&quot;math/tex&quot;&gt;\theta\in\Theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Find the “true” parameters &lt;script type=&quot;math/tex&quot;&gt;\theta^* \in\Theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt;: The idea is to find a set of &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; constraints, or “moments”, involving the parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. What makes GMMs nice is that you need no information per say about how the model depends on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. Certainly they can be used to construct moments (special case: maximum likelihood estimation (MLE)), but one can use, for example, statistical moments (special case: method of moments (MoM)) as the constraints. Analogously, tensor decompositions are used in the case of spectral methods.&lt;/p&gt;

&lt;p&gt;More formally, the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; &lt;strong&gt;moment conditions&lt;/strong&gt; for a vector-valued function &lt;script type=&quot;math/tex&quot;&gt;g(Y,\cdot):\Theta\to\mathbb{R}^k&lt;/script&gt; is&lt;/p&gt;

&lt;p&gt;[
m(\theta^* ) \equiv \mathbb{E}[g(Y,\theta^* )] = 0_{k\times 1},
]&lt;/p&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;0_{k\times 1}&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;k\times 1&lt;/script&gt; zero vector.&lt;/p&gt;

&lt;p&gt;As we cannot analytically derive the expectation for arbitrary &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt;, we use the sample moments instead:&lt;/p&gt;

&lt;p&gt;[
\hat m(\theta) \equiv \frac{1}{N}\sum_{n=1}^N g(Y_n,\theta)
]&lt;/p&gt;

&lt;p&gt;By the Law of Large Numbers, &lt;script type=&quot;math/tex&quot;&gt;\hat{m}(\theta)\to m(\theta)&lt;/script&gt;, so the problem is thus to find the &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; which sets &lt;script type=&quot;math/tex&quot;&gt;\hat m(\theta)&lt;/script&gt; to zero.&lt;/p&gt;

&lt;p&gt;Cases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Theta\supset\mathbb{R}^k&lt;/script&gt;, i.e., there are more parameters than moment
conditions: The model is not &lt;a href=&quot;http://en.wikipedia.org/wiki/Identifiability&quot;&gt;identifiable&lt;/a&gt;. This is the standard scenario in ordinary least squares (OLS) when there are more covariates than observations and so no unique set of parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; exist. Solve this by simply constructing more moments!&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Theta=\mathbb{R}^k&lt;/script&gt;: There exists a unique solution.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Theta\subset\mathbb{R}^k&lt;/script&gt;,
i.e., there are fewer parameters than moment conditions: The parameters are overspecified and the best we can do is to minimize &lt;script type=&quot;math/tex&quot;&gt;m(\theta)&lt;/script&gt; instead of solve &lt;script type=&quot;math/tex&quot;&gt;m(\theta)=0&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Consider the last scenario: we aim to minimize &lt;script type=&quot;math/tex&quot;&gt;\hat m(\theta)&lt;/script&gt; in some way, say &lt;script type=&quot;math/tex&quot;&gt;\|\hat m(\theta)\|&lt;/script&gt; for some choice of &lt;script type=&quot;math/tex&quot;&gt;\|\cdot\|&lt;/script&gt;. We define the &lt;strong&gt;weighted norm&lt;/strong&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|\hat m(\theta)\|_W^2 \equiv \hat m(\theta)^T W \hat m(\theta),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; is a positive definite matrix.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;generalized method of moments&lt;/strong&gt; (GMMs) procedure is to find&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\theta = {arg\ min}_{\theta\in\Theta}
\left(\frac{1}{N}\sum_{n=1}^N g(Y_n,\theta)\right)^T W
\left(\frac{1}{N}\sum_{n=1}^N g(Y_n,\theta)\right)&lt;/script&gt;

&lt;p&gt;Note that while the motivation is for &lt;script type=&quot;math/tex&quot;&gt;\theta\supset\mathbb{R}^k&lt;/script&gt;, by the unique solution, this is guaranteed to work for &lt;script type=&quot;math/tex&quot;&gt;\Theta=\mathbb{R}^k&lt;/script&gt; too. Hence it is a &lt;em&gt;generalized&lt;/em&gt; method of moments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;. Under standard assumptions¹, the estimator &lt;script type=&quot;math/tex&quot;&gt;\hat\theta&lt;/script&gt; is &lt;a href=&quot;http://en.wikipedia.org/wiki/Consistent_estimator#Bias_versus_consistency&quot;&gt;consistent&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Asymptotic_distribution&quot;&gt;asymptotically normal&lt;/a&gt;. Furthermore, if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W \propto
\Omega^{-1}\equiv\mathbb{E}[g(Y_n,\theta^*)g(Y_n,\theta^*)^T]^{-1}&lt;/script&gt;

&lt;p&gt;then &lt;script type=&quot;math/tex&quot;&gt;\hat \theta&lt;/script&gt; is &lt;a href=&quot;http://en.wikipedia.org/wiki/Efficiency_(statistics)&quot;&gt;asymptotically optimal&lt;/a&gt;, i.e., achieves the Cramér-Rao lower bound.&lt;/p&gt;

&lt;p&gt;Note that &lt;script type=&quot;math/tex&quot;&gt;\Omega&lt;/script&gt; is the covariance matrix of &lt;script type=&quot;math/tex&quot;&gt;g(Y_n,\theta^*)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\Omega^{-1}&lt;/script&gt; the precision. Thus the GMM weights the parameters of the estimator &lt;script type=&quot;math/tex&quot;&gt;\hat\theta&lt;/script&gt; depending on how much “error” remains in &lt;script type=&quot;math/tex&quot;&gt;g(Y,\cdot)&lt;/script&gt; per parameter of &lt;script type=&quot;math/tex&quot;&gt;\theta^*&lt;/script&gt; (that is, how far away &lt;script type=&quot;math/tex&quot;&gt;g(Y,\cdot)&lt;/script&gt; is from 0).&lt;/p&gt;

&lt;p&gt;I haven’t seen anyone make this remark before, but the GMM estimator can also be viewed as minimizing a log-normal quantity. Recall that the multivariate normal distribution is proportional to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\exp\Big((Y_n-\mu)^T\Sigma^{-1}(Y_n-\mu)\Big)&lt;/script&gt;

&lt;p&gt;Setting &lt;script type=&quot;math/tex&quot;&gt;g(Y_n,\theta)\equiv Y_n-\mu&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W\equiv\Sigma&lt;/script&gt;, and taking the log, this is exactly the expression for the GMM! By the asymptotic normality, this explains why would want to set &lt;script type=&quot;math/tex&quot;&gt;W\equiv\Sigma&lt;/script&gt; in order to achieve statistical efficiency.&lt;/p&gt;

&lt;p&gt;¹ The standard assumptions can be found in [1]. In practice they will almost always be satisfied, e.g., compact parameter space, &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; is continuously differentiable in a neighborhood of &lt;script type=&quot;math/tex&quot;&gt;\theta^*&lt;/script&gt;, output of &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; is never infinite, etc.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Alastair Hall. &lt;em&gt;Generalized Method of Moments (Advanced Texts in Econometrics)&lt;/em&gt;. Oxford University Press, 2005.&lt;/p&gt;

&lt;p&gt;@inproceedings{tran2017deep,
  author = {Dustin Tran and Matthew D. Hoffman and Rif A. Saurous and Eugene Brevdo and Kevin Murphy and David M. Blei},
  title = {Deep probabilistic programming},
  booktitle = {International Conference on Learning Representations},
  year = {2017}
}&lt;/p&gt;
</description>
        
        <pubDate>Mon, 01 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/2018/01/01/deep-prob-modeling/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2018/01/01/deep-prob-modeling/</guid>
      </item>
    
      <item>
        <title>What's Jekyll?</title>
        
          <description>&lt;p&gt;&lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; is a static site generator, an open-source tool for creating simple yet powerful websites of all shapes and sizes. From &lt;a href=&quot;https://github.com/mojombo/jekyll/blob/master/README.markdown&quot;&gt;the project’s readme&lt;/a&gt;:&lt;/p&gt;

&lt;!--more--&gt;

&lt;blockquote&gt;
  &lt;p&gt;Jekyll is a simple, blog aware, static site generator. It takes a template directory […] and spits out a complete, static website suitable for serving with Apache or your favorite web server. This is also the engine behind GitHub Pages, which you can use to host your project’s page or blog right here from GitHub.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It’s an immensely useful tool and one we encourage you to use here with Lanyon.&lt;/p&gt;

&lt;p&gt;Find out more by &lt;a href=&quot;https://github.com/mojombo/jekyll&quot;&gt;visiting the project on GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
        
        <pubDate>Sat, 31 Jan 2015 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/2015/01/31/whats-jekyll/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2015/01/31/whats-jekyll/</guid>
      </item>
    
      <item>
        <title>Introducing Lanyon</title>
        
          <description>&lt;p&gt;Lanyon is an unassuming &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; theme that places content first by tucking away navigation in a hidden drawer. It’s based on &lt;a href=&quot;http://getpoole.com&quot;&gt;Poole&lt;/a&gt;, the Jekyll butler.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;built-on-poole&quot;&gt;Built on Poole&lt;/h3&gt;

&lt;p&gt;Poole is the Jekyll Butler, serving as an upstanding and effective foundation for Jekyll themes by &lt;a href=&quot;https://twitter.com/mdo&quot;&gt;@mdo&lt;/a&gt;. Poole, and every theme built on it (like Lanyon here) includes the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Complete Jekyll setup included (layouts, config, &lt;a href=&quot;/404&quot;&gt;404&lt;/a&gt;, &lt;a href=&quot;/atom.xml&quot;&gt;RSS feed&lt;/a&gt;, posts, and &lt;a href=&quot;/about&quot;&gt;example page&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Mobile friendly design and development&lt;/li&gt;
  &lt;li&gt;Easily scalable text and component sizing with &lt;code class=&quot;highlighter-rouge&quot;&gt;rem&lt;/code&gt; units in the CSS&lt;/li&gt;
  &lt;li&gt;Support for a wide gamut of HTML elements&lt;/li&gt;
  &lt;li&gt;Related posts (time-based, because Jekyll) below each post&lt;/li&gt;
  &lt;li&gt;Syntax highlighting, courtesy Pygments (the Python-based code snippet highlighter)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lanyon-features&quot;&gt;Lanyon features&lt;/h3&gt;

&lt;p&gt;In addition to the features of Poole, Lanyon adds the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Toggleable sliding sidebar (built with only CSS) via &lt;strong&gt;☰&lt;/strong&gt; link in top corner&lt;/li&gt;
  &lt;li&gt;Sidebar includes support for textual modules and a dynamically generated navigation with active link support&lt;/li&gt;
  &lt;li&gt;Two orientations for content and sidebar, default (left sidebar) and &lt;a href=&quot;https://github.com/poole/lanyon#reverse-layout&quot;&gt;reverse&lt;/a&gt; (right sidebar), available via &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;body&amp;gt;&lt;/code&gt; classes&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/poole/lanyon#themes&quot;&gt;Eight optional color schemes&lt;/a&gt;, available via &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;body&amp;gt;&lt;/code&gt; classes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/poole/lanyon#readme&quot;&gt;Head to the readme&lt;/a&gt; to learn more.&lt;/p&gt;

&lt;h3 id=&quot;browser-support&quot;&gt;Browser support&lt;/h3&gt;

&lt;p&gt;Lanyon is by preference a forward-thinking project. In addition to the latest versions of Chrome, Safari (mobile and desktop), and Firefox, it is only compatible with Internet Explorer 9 and above.&lt;/p&gt;

&lt;h3 id=&quot;download&quot;&gt;Download&lt;/h3&gt;

&lt;p&gt;Lanyon is developed on and hosted with GitHub. Head to the &lt;a href=&quot;https://github.com/poole/lanyon&quot;&gt;GitHub repository&lt;/a&gt; for downloads, bug reports, and features requests.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
</description>
        
        <pubDate>Thu, 02 Jan 2014 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/2014/01/02/introducing-lanyon/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2014/01/02/introducing-lanyon/</guid>
      </item>
    
      <item>
        <title>Example content</title>
        
          <description>&lt;p&gt;Howdy! This is an example blog post that shows features supported in &lt;strong&gt;lanyon-plus&lt;/strong&gt; theme. See &lt;a href=&quot;https://raw.githubusercontent.com/dyndna/lanyon-plus/master/_posts/2013-01-01-example-content.md&quot;&gt;raw post&lt;/a&gt; for required YAML header and liquid tag specifications.&lt;/p&gt;

&lt;!--more--&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#heading&quot; id=&quot;markdown-toc-heading&quot;&gt;Heading&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#inline-html-elements&quot; id=&quot;markdown-toc-inline-html-elements&quot;&gt;Inline HTML elements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#code&quot; id=&quot;markdown-toc-code&quot;&gt;Code&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#alternate-code-block&quot; id=&quot;markdown-toc-alternate-code-block&quot;&gt;Alternate code block:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pygments-based-highlighting-deprecated&quot; id=&quot;markdown-toc-pygments-based-highlighting-deprecated&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pygments&lt;/code&gt; based highlighting (deprecated)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lists&quot; id=&quot;markdown-toc-lists&quot;&gt;Lists&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tables&quot; id=&quot;markdown-toc-tables&quot;&gt;Tables&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#markdown-formatted&quot; id=&quot;markdown-toc-markdown-formatted&quot;&gt;markdown formatted:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#html-table&quot; id=&quot;markdown-toc-html-table&quot;&gt;html table:&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#text-highlight&quot; id=&quot;markdown-toc-text-highlight&quot;&gt;Text highlight:&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#blockquotes&quot; id=&quot;markdown-toc-blockquotes&quot;&gt;Blockquotes&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#stylized&quot; id=&quot;markdown-toc-stylized&quot;&gt;stylized&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#regular&quot; id=&quot;markdown-toc-regular&quot;&gt;regular&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#embed-video&quot; id=&quot;markdown-toc-embed-video&quot;&gt;Embed video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#embed-picture&quot; id=&quot;markdown-toc-embed-picture&quot;&gt;Embed picture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#inline-css-attributes&quot; id=&quot;markdown-toc-inline-css-attributes&quot;&gt;Inline css attributes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mathjax&quot; id=&quot;markdown-toc-mathjax&quot;&gt;MathJax&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gist-embed&quot; id=&quot;markdown-toc-gist-embed&quot;&gt;gist embed&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot; id=&quot;markdown-toc-references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heading&quot;&gt;Heading&lt;/h3&gt;

&lt;h1 class=&quot;no_toc&quot; id=&quot;h1&quot;&gt;H1&lt;/h1&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;h2&quot;&gt;H2&lt;/h2&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;h3&quot;&gt;H3&lt;/h3&gt;

&lt;h4 class=&quot;no_toc&quot; id=&quot;h4&quot;&gt;H4&lt;/h4&gt;

&lt;h5 class=&quot;no_toc&quot; id=&quot;h5&quot;&gt;H5&lt;/h5&gt;

&lt;h6 class=&quot;no_toc&quot; id=&quot;h6&quot;&gt;H6&lt;/h6&gt;

&lt;p&gt;Vivamus sagittis lacus vel augue rutrum faucibus dolor auctor. Duis mollis, est non commodo luctus, nisi erat porttitor ligula, eget lacinia odio sem nec elit. Morbi leo risus, porta ac consectetur ac, vestibulum at eros.&lt;/p&gt;

&lt;h3 id=&quot;inline-html-elements&quot;&gt;Inline HTML elements&lt;/h3&gt;

&lt;p&gt;HTML defines a long list of available inline tags, a complete list of which can be found on the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/HTML/Element&quot;&gt;Mozilla Developer Network&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;To bold text&lt;/strong&gt;, use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;strong&amp;gt;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;To italicize text&lt;/em&gt;, use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;em&amp;gt;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Abbreviations, like &lt;abbr title=&quot;HyperText Markup Langage&quot;&gt;HTML&lt;/abbr&gt; should use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;abbr&amp;gt;&lt;/code&gt;, with an optional &lt;code class=&quot;highlighter-rouge&quot;&gt;title&lt;/code&gt; attribute for the full phrase.&lt;/li&gt;
  &lt;li&gt;Citations, like &lt;cite&gt;— Mark otto&lt;/cite&gt;, should use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;cite&amp;gt;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;del&gt;Deleted&lt;/del&gt; text should use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;del&amp;gt;&lt;/code&gt; and &lt;ins&gt;inserted&lt;/ins&gt; text should use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;ins&amp;gt;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Superscript &lt;sup&gt;text&lt;/sup&gt; uses &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;sup&amp;gt;&lt;/code&gt; and subscript &lt;sub&gt;text&lt;/sub&gt; uses &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;sub&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of these elements are styled by browsers with few modifications on our part.&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;

&lt;p&gt;Cum sociis natoque penatibus et magnis dis &lt;code class=&quot;highlighter-rouge&quot;&gt;code element&lt;/code&gt; montes, nascetur ridiculus mus.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// Example can be run directly in your JavaScript console&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Create a function that takes two arguments and returns the sum of those arguments&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;adder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;return a + b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Call the function&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;adder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// &amp;gt; 8&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Aenean lacinia bibendum nulla sed consectetur. Etiam porta sem malesuada magna mollis euismod. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa.&lt;/p&gt;

&lt;h4 id=&quot;alternate-code-block&quot;&gt;Alternate code block:&lt;/h4&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uuid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stringr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;pygments-based-highlighting-deprecated&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pygments&lt;/code&gt; based highlighting (deprecated)&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;library&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;uuid&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
library&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;stringr&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
library&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;parallel&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
library&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;data.table&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;lists&quot;&gt;Lists&lt;/h3&gt;

&lt;p&gt;Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aenean lacinia bibendum nulla sed consectetur. Etiam porta sem malesuada magna mollis euismod. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Praesent commodo cursus magna, vel scelerisque nisl consectetur et.&lt;/li&gt;
  &lt;li&gt;Donec id elit non mi porta gravida at eget metus.&lt;/li&gt;
  &lt;li&gt;Nulla vitae elit libero, a pharetra augue.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Donec ullamcorper nulla non metus auctor fringilla. Nulla vitae elit libero, a pharetra augue.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Vestibulum id ligula porta felis euismod semper.&lt;/li&gt;
  &lt;li&gt;Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.&lt;/li&gt;
  &lt;li&gt;Maecenas sed diam eget risus varius blandit sit amet non magna.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cras mattis consectetur purus sit amet fermentum. Sed posuere consectetur est at lobortis.&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;HyperText Markup Language (HTML)&lt;/dt&gt;
  &lt;dd&gt;The language used to describe and define the content of a Web page&lt;/dd&gt;

  &lt;dt&gt;Cascading Style Sheets (CSS)&lt;/dt&gt;
  &lt;dd&gt;Used to describe the appearance of Web content&lt;/dd&gt;

  &lt;dt&gt;JavaScript (JS)&lt;/dt&gt;
  &lt;dd&gt;The programming language used to build advanced Web sites and applications&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Integer posuere erat a ante venenatis dapibus posuere velit aliquet. Morbi leo risus, porta ac consectetur ac, vestibulum at eros. Nullam quis risus eget urna mollis ornare vel eu leo.&lt;/p&gt;

&lt;h3 id=&quot;tables&quot;&gt;Tables&lt;/h3&gt;

&lt;h4 id=&quot;markdown-formatted&quot;&gt;markdown formatted:&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Heading 1&lt;/th&gt;
      &lt;th&gt;Heading 2&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Heading 3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Hello&lt;/td&gt;
      &lt;td&gt;World&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;i class=&quot;fa fa-twitter&quot;&gt; @foo&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Aenean lacinia bibendum nulla sed consectetur. Lorem ipsum dolor sit amet, consectetur adipiscing elit.&lt;/p&gt;

&lt;h4 id=&quot;html-table&quot;&gt;html table:&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Upvotes&lt;/th&gt;
      &lt;th&gt;Downvotes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tfoot&gt;
    &lt;tr&gt;
      &lt;td&gt;Totals&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tfoot&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Alice&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bob&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Charlie&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;text-highlight&quot;&gt;Text highlight:&lt;/h3&gt;

&lt;p&gt;Nullam id dolor id nibh ultricies vehicula ut id elit. Sed posuere consectetur est at lobortis. Nullam quis risus eget urna mollis ornare vel eu leo. &lt;code class=&quot;yelhglt highlighter-rouge&quot;&gt;Nullam id dolor id nibh ultricies vehicula ut id elit. Highlighted color will be removed during prinintg and replaced with underline.&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;blockquotes&quot;&gt;Blockquotes&lt;/h3&gt;

&lt;h4 id=&quot;stylized&quot;&gt;stylized&lt;/h4&gt;

&lt;blockquote class=&quot;style1&quot;&gt;
Aenean lacinia bibendum nulla sed consectetur. Etiam porta sem malesuada magna mollis euismod.
&lt;/blockquote&gt;

&lt;h4 id=&quot;regular&quot;&gt;regular&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;When you bring new things into a society, you can either, it’s like the balance of the force. You can either use it for good or you can use it for evil. And what happens when something new: People have a tendency to overdo it; they abuse it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;embed-video&quot;&gt;Embed video&lt;/h3&gt;

&lt;hr class=&quot;transp&quot; /&gt;

&lt;div class=&quot;ytube-video-container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/iG9CE55wbtY&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;hr class=&quot;transp&quot; /&gt;

&lt;h3 id=&quot;embed-picture&quot;&gt;Embed picture&lt;/h3&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;https://dyndna.github.io/lanyon-plus/images//media/apple-icon.png&quot; alt=&quot;Toy example&quot; title=&quot;Toy example&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;inline-css-attributes&quot;&gt;Inline css attributes&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://haixing-hu.github.io/programming/2013/09/20/how-to-use-mathjax-in-jekyll-generated-github-pages/&quot; target=&quot;_blank&quot;&gt;MahJax source - link open in new window&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;mathjax&quot;&gt;MathJax&lt;/h3&gt;

&lt;p&gt;Let’s test some inline math $x$, $y$, $x_1$, $y_1$.&lt;/p&gt;

&lt;p&gt;Now a inline math with special character: $|\psi\rangle$, $x’$, $x^*$ and $|\psi_1\rangle = a|0\rangle + b|1\rangle$&lt;/p&gt;

&lt;p&gt;Test a display math:
&lt;script type=&quot;math/tex&quot;&gt;|\psi_1\rangle = a|0\rangle + b|1\rangle&lt;/script&gt;
Is it O.K.?&lt;/p&gt;

&lt;p&gt;Test a display math with equation number:
\begin{equation}
   |\psi_1\rangle = a|0\rangle + b|1\rangle
\end{equation}
Is it O.K.?&lt;/p&gt;

&lt;p&gt;Test a display math with equation number:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
    |\psi_1\rangle &amp;= a|0\rangle + b|1\rangle \\
    |\psi_2\rangle &amp;= c|0\rangle + d|1\rangle
  \end{align} %]]&gt;&lt;/script&gt;
Is it O.K.?&lt;/p&gt;

&lt;p&gt;And test a display math without equaltion number:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
    |\psi_1\rangle &amp;= a|0\rangle + b|1\rangle \\
    |\psi_2\rangle &amp;= c|0\rangle + d|1\rangle
  \end{align*} %]]&gt;&lt;/script&gt;
Is it O.K.?&lt;/p&gt;

&lt;p&gt;Test a display math with equation number:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
    |\psi_1\rangle &amp;= a|0\rangle + b|1\rangle \\
    |\psi_2\rangle &amp;= c|0\rangle + d|1\rangle
\end{align} %]]&gt;&lt;/script&gt;
Is it O.K.?&lt;/p&gt;

&lt;p&gt;And test a display math without equaltion number:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
    |\psi_1\rangle &amp;= a|0\rangle + b|1\rangle \\
    |\psi_2\rangle &amp;= c|0\rangle + d|1\rangle
\end{align*} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;gist-embed&quot;&gt;gist embed&lt;/h3&gt;

&lt;p&gt;Below is a partial code showing main steps of merge function.&lt;/p&gt;

&lt;p&gt;&lt;code data-gist-id=&quot;0fe211678316cc53370c&quot; data-gist-file=&quot;merge_tables_datatable.R&quot; data-gist-line=&quot;50-52,57,65-69,80,88-90,100-106&quot;&gt;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;Multiple&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and with comments&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Want to see something else added? &lt;a href=&quot;https://github.com/dyndna/lanyon-plus/issues/new&quot;&gt;Open an issue.&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://lanyon.getpoole.com&quot;&gt;lanyon theme&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://localhost:4000/about&quot;&gt;About&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;

      &lt;p&gt;&lt;a href=&quot;https://github.com/dyndna/lanyon-plus&quot; title=&quot;accessed on January 01, 2013&quot;&gt;lanyon-plus theme&lt;/a&gt;&lt;/p&gt;

      &lt;blockquote&gt;
        &lt;p&gt;Excerpt: Sample post showing enabled features for post: &lt;code class=&quot;highlighter-rouge&quot;&gt;inline code&lt;/code&gt;, &lt;code class=&quot;yelhglt highlighter-rouge&quot;&gt;text highlight&lt;/code&gt;, code block with syntax highlights, embed gist, stylized blockquotes, video and image cards for twitter, markdown tables, inline and code bocks having mathjax support, references, print format, disqus comments, related posts, tags.&lt;/p&gt;
      &lt;/blockquote&gt;
      &lt;p&gt;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        
        <pubDate>Tue, 01 Jan 2013 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/2013/01/01/example_content/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2013/01/01/example_content/</guid>
      </item>
    
  </channel>
</rss>
